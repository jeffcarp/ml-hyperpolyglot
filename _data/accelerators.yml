- name: General
  comparisons:
  - label: Release Year
    frameworks:
      tpuv3: "2018"
      tpuv4: "2021"
      tpuv5e: "2023"
      tpuv5p: "2023"
      tpuv6e: "2024"
      tpu7x: "2025"
      v100: "2017"
      a100: "2020"
      h100: "2022"
      h200: "2023"
      b200: "2024"
  - label: Architecture
    frameworks:
      tpuv3: "MXU"
      tpuv4: "MXU"
      tpuv5e: "TensorCore"
      tpuv5p: "TensorCore"
      tpuv6e: "Trillium"
      tpu7x: "Ironwood"
      v100: "Volta"
      a100: "Ampere"
      h100: "Hopper"
      h200: "Hopper"
      b200: "Blackwell"

- name: Memory
  comparisons:
  - label: HBM Capacity
    frameworks:
      tpuv3: "32 GB"
      tpuv4: "32 GB"
      tpuv5e: "16 GB"
      tpuv5p: "95 GB"
      tpuv6e: "32 GB"
      tpu7x: "192 GB"
      v100: "32 GB"
      a100: "80 GB"
      h100: "80 GB"
      h200: "141 GB"
      b200: "192 GB"
  - label: HBM Bandwidth (GB/s)
    frameworks:
      tpuv3: "900"
      tpuv4: "1200"
      tpuv5e: "819"
      tpuv5p: "2765"
      tpuv6e: "1638"
      tpu7x: "7380"
      v100: "900"
      a100: "2039"
      h100: "3350"
      h200: "4800"
      b200: "8000"
  - label: SMEM/VMEM Capacity
    frameworks:
      tpuv3: "16 MiB (per core)"
      tpuv4: "16 MiB (per core)"
      tpuv5e: "128 MiB"
      tpuv5p:
        comment: (unpublished)
      tpuv6e:
        comment: (unpublished)
      tpu7x:
        comment: (unpublished)
      v100: "6 MiB"
      a100: "40 MiB"
      h100: "50 MiB"
      h200: "60 MiB"
      b200: ""
    remarks: |
      - **TPU VMEM**: On-chip Vector Memory (SRAM) local to each TensorCore. Values are per-core unless shared.
      - **GPU L2**: For GPUs, the L2 cache is the closest architectural equivalent to the TPU's on-chip scratchpad memory.

- name: Compute (Peak)
  comparisons:
  - label: BF16 / FP16 TFLOPS
    frameworks:
      tpuv3: "123"
      tpuv4: "275"
      tpuv5e: "197"
      tpuv5p: "459"
      tpuv6e: "918"
      tpu7x: "2307"
      v100: "125"
      a100: "312"
      h100: "1979"
      h200: "1979"
      b200: "2250"
    remarks: |
      - **TPU7x/Ironwood**: First dual-chiplet architecture, exposing 2 devices per chip.
      - **Nvidia Tensor Cores**: Figures are for Tensor Core performance (with sparsity where applicable for newer gens).
  - label: FP8 TFLOPS
    frameworks:
      tpuv3: ""
      tpuv4: ""
      tpuv5e: "197"
      tpuv5p: "459"
      tpuv6e: "918"
      tpu7x: "4614"
      v100: ""
      a100: ""
      h100: "3958"
      h200: "3958"
      b200: "4500"
  - label: INT8 TOPS
    frameworks:
      tpuv3: ""
      tpuv4: "275"
      tpuv5e: "393"
      tpuv5p: "918"
      tpuv6e: "1836"
      tpu7x: ""
      v100: "62"
      a100: "624"
      h100: "3958"
      h200: "3958"
      b200: "4500"
  - label: FP4 TFLOPS
    frameworks:
      tpuv3: ""
      tpuv4: ""
      tpuv5e: ""
      tpuv5p: ""
      tpuv6e: ""
      tpu7x: ""
      v100: ""
      a100: ""
      h100: ""
      h200: ""
      b200: "9000"

- name: Interconnect & Scale
  comparisons:
  - label: ICI Bandwidth (Per Chip) (GB / s)
    frameworks:
      tpuv3: 656
      tpuv4: 300
      tpuv5e: 200
      tpuv5p: 1200
      tpuv6e: 400
      tpu7x:
      v100: 300
      a100: 600
      h100: 900
      h200: 900
      b200: 1800
  - label: DCN Bandwidth (Per Chip)
    frameworks:
      tpuv3: "~10 GB/s (100 Gbps)"
      tpuv4: "~12.5 GB/s (100 Gbps)"
      tpuv5e: "~12.5 GB/s (100 Gbps)"
      tpuv5p: "~25 GB/s (200 Gbps)"
      tpuv6e: "~25 GB/s (200 Gbps)"
      tpu7x: ""
      v100: "~12.5 GB/s (100 Gbps)"
      a100: "~25 GB/s (200 Gbps)"
      h100: "~50 GB/s (400 Gbps)"
      h200: "~50 GB/s (400 Gbps)"
      b200: "~50-100 GB/s (400-800 Gbps)"
    remarks: |
      - **DCN (Data Center Network)**: Network bandwidth for inter-rack communication, typically Ethernet or InfiniBand. Values are approximate and vary by deployment.
  - label: Max Scale (ICI Domain)
    frameworks:
      tpuv3: "1024 chips"
      tpuv4: "4096 chips"
      tpuv5e: "256 chips"
      tpuv5p: "8960 chips"
      tpuv6e: "256 chips"
      tpu7x: "9216 chips"
      v100: "8 (Server)"
      a100: "16 (NVSwitch)"
      h100: "256 (SuperPOD)"
      h200: "256 (SuperPOD)"
      b200: "576 (NVLink)"
    remarks: |
      - **ICI (Inter-Chip Interconnect)**: Dedicated fabric for chip-to-chip communication (TPU ICI or Nvidia NVLink).
      - **ICI Domain**: Maximum number of chips connectable via the specialized low-latency fabric before requiring Ethernet/InfiniBand.
      - **TPU v4/v5p**: Feature 3D Torus interconnects for massive scale.
      - **[TPU 7x docs](https://docs.cloud.google.com/tpu/docs/tpu7x)**
      - **Nvidia NVLink**: Historically server-scale, now scaling to hundreds with NVSwitch (e.g., NVL72/576).
